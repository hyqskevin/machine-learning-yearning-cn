---
title: 优化验证测试
permalink: /docs/ch44/
---

假设你正在建立一个语音识别系统。系统通过输入一段音频A，同时为每个可能的输出语句S计算得分 $Score_A(S)$。例如，你可能会尝试估计 $Score_A(S)=P(S|A)$，即假定输入音频是A，求能正确输出转换语句S的概率。

现在给定一种计算$Score_A(S)$的方法，你需要不断地找到输出的英语句子S使得以下概率最大化：
  $$Output=argmax_s Score_A(S)$$

如何计算上面的“arg max”？如果英文有5万个单词，那么长度为N的句子将组成$(50000)^N$种可能—数量庞大到难以穷举。所以你需要一个近似搜索算法去尝试找到值S来优化（最大化）$Score_A(S)$。一个搜索算法的例子是“集束搜索”(beam search)，即在搜索过程中保留K个最佳候选项。（就本章而言，你不需要了解集束搜索的细节。）但这样的算法并不能保证找到使得$Score_A(S)$最大化的S的值。

假设一个音频片段A记录某人说“我爱机器学习”，但你的系统并没有正确的转换输出，而是错误的“我爱机器人”。这里有两种出错的可能性：

1. **搜索算法问题**：近似搜索算法没有找到值S使$Score_A(S)$最大化

2. **目标（评分函数）问题**：我们对$Score_A(S)=P(S|A)$的估计不准确。在特殊情况下，我们的选择的$Score_A(S)$没有识别出“我爱机器学习”是正确的转换。

根据这些失败的原因，你需要优先区分你的努力方向。如果是问题1,你需要努力改进搜索算法。如果是问题2，你需要改进用于评估$Score_A(S)$的学习算法。

面对这种情况，一些研究人员会随机决定使用搜索算法；还有人会随机决定使用更好的方式去学习$Score_A(S)$的值。但是除非你知道哪些是导致错误的根本原因，否则你的努力就可能白费。那么，如何更系统地决定你要做什么呢？

设$S_{out}$为输出转换（“我爱机器人”），$S*$为正确的转换（“我爱机器学习”）。为了确认到底是问题1还是问题2，你可以进行**优化验证测试(Optimization Verification test)** ：首先，计算$Score_A(S*)$和$Score_A(S_{out})$。然后检查是否$Score_A(S*) > Score_A(S_{out})$。有两种可能：

情况1：$Score_A(S*) > Score_A(S_{out})$

在这种情况下，你的学习算法给出$S*$分数比$S_{out}$高，然而近似选择算法选择了$S_{out}$而不是$S*$。这表明近似搜索算法未能选择值S使$Score_A(S*)$最大化。优化验证测试会告诉你搜索算法的问题，应该着重解决。例如，你可以尝试增加集束搜索的集束宽度。

情况2：$Score_A(S*) <= Score_A(S_{out})$

在这种情况下，你知道计算$Score_A(.)$的方式出现了错误——对错误输出$S_{out}$，无法给出分数更高的正确输出$S*$。优化验证测试表明你有一个目标（评分）函数问题。因此对于不同语句S，你需要专注去提升学习方法或接近$Score_A(S*)$。

我们来重点讨论一个例子，为了在实践中运用优化验证测试，你需要检查在开发集上的错误。对于每个错误，你要测试是否满足$Score_A(S*) > Score_A(S_{out})$。每一个不等式成立例子都要标记为优化算法引起的错误；每一个不成立的例子（$Score_A(S*) <= Score_A(S_{out})$），都算作计算$Score_A(.)$时的错误。

比如，假设你发现95%的错误是由评分函数$Score_A(.)$引起的,优化算法只占到5%，不管你改进多少优化程序，实际山只减少了5%的错误。因此你应该重点改进评估$Score_A(.)$的方法。
