---
title: 使用单值评估指标进行优化
permalink: /docs/ch08/
---

所谓的**单值评估指标（single-number evaluation metric）**有很多，分类准确率就是其中的一种：你在开发集（或测试集）上运行分类器后，它将返回单个数值，代表着被正确分类的样本比例。根据这个指标，如果分类器 A 的准确率为 97％，而分类器 B 的准确率为 90%，那么我们可以认为分类器 A 更优秀。

相比之下，**查准率**（Precision，又译作精度）和**查全率**（Recall，又译作召回率）不是单值评估指标，因为它给出了两个值来对你的分类器进行评估。多值评估指标将使算法之间的优劣比较变得更加困难，假设你的算法表现如下：

| Classifier | Precision | Recall |
| ---------- | --------- | ------ |
| A          | 95%       | 90%    |
| B          | 98%       | 85%    |

> 猫分类器的查准率指的是在开发集（或者测试集）被预测为猫的图片中，实际类别是猫的样本比例。而查全率指的是在开发集（或者测试集）所有实际类别为猫的图片中，被正确预测为猫的样本比例。人们常常在查准率和查全率之间权衡取舍。

根据上面表格的数值进行比较，两个分类器都没有较为明显的优势，因此也无法引导你立即做出选择。

| Classifier | Precision | Recall | F1 score  |
| ---------- | --------- | ------ | --------- |
| A          | 95%       | 90%    | **92.4%** |

你的团队在进行开发时往往会尝试许多的算法架构、模型参数、特征选择，或者是其它的想法。使用单值评估指标（如准确率）可以让你将所有的模型根据在此指标上的表现进行排序，从而快速确定哪一个模型的性能表现最好。

如果你认为查准率和查全率很关键，可以参考其他人的做法，将这两个值合并为一个值来表示。例如取二者的平均值，或者你可以计算 “F1分数（F1 score）” ，这是一种经过修正的平均值计算方法，比进行简单取平均的效果会好一些。

| Classifier | Precision | Recall | F1 score  |
| ---------- | --------- | ------ | --------- |
| A          | 95%       | 90%    | **92.4%** |
| B          | 98%       | 85%    | **91.0%** |

> 如果你想了解更多关于 F1 分数的信息，可以参考  **https://en.wikipedia.org/wiki/F1_score** 它是查准率和查全率的调和平均数，计算公式为2/((1/Precision)+(1/Recall))。

因此，当你在多个分类器之间进行抉择时，使用单值评估指标将帮助你更快速地作出决定。它能给出一个清楚的分类器性能排名，从而帮助明确团队后续的处理方向。

最后补充一个例子，假设你在“美国”、“印度”、“中国”和“其它地区”，这四个关键市场跟踪你的猫分类器的准确率，并且获得了四个指标。通过对这四个指标取平均值或进行加权平均，你将得到一个单值指标。**取平均值或者加权平均值是将多个指标合并为一个指标的最常用方法之一。**
