---
title: 优化指标和满意度指标
permalink: /docs/ch09/
---

下面将提到组合多个评估指标的另一种方法。

假设你既关心学习算法的准确率（accuracy），又在意其运行时间（running time），请从下面的三个分类器中进行选择：

| Classifier | Accuracy | Running time |
| ---------- | -------- | ------------ |
| A          | 90%      | 80ms         |
| B          | 92%      | 95ms         |
| C          | 95%      | 1,500ms      |

将准确率和与运行时间放入单个公式计算后可以导出单个的指标，这似乎不太自然，例如：

$$
Accuracy - 0.5 * RunningTime
$$

你可以有一种替代的方案：首先定义一个“可接受的”运行时间，一般低于 100ms 。接着在限定的运行时间范围内最大化分类器的准确率。此处的运行时间是一个“满意度指标” —— 你的分类器必须在这个指标上表现得“足够好”，这儿指的是它应该至多需要 100ms，而准确度是一个“优化指标”。

如果考虑 N 项不同的标准，比如模型的二进制文件大小（这对移动端 app 尤为重要，因为用户不想下载体积很大的 app）、运行时间和准确率。你或许会考虑设置 N-1 个“满意度”指标，即要求它们满足一定的值，下一步才是定义一个“优化”指标。例如为二进制文件的大小和运行时间分别设定可接受的阈值，并尝试根据这些限制来优化准确率指标。

最后再举一个例子，假设你正在设计一个硬件设备，该设备可以根据用户设置的特定“唤醒词”来唤醒系统，类似于Amazon Echo 监听词为 “Alexa”，苹果（Apple） Siri 监听词为 “Hey Siri”，安卓（Android） 监听词为 “Okay Google”，以及百度（Baidu）应用监听 “Hello Baidu.” 我们关心的是假正例率（false positive rate）—— 用户没有说出唤醒词，系统却被唤醒了，以及假反例率（false negative rate）——用户说出了唤醒词，系统却没能正确被唤醒。这个系统的一个较为合理的优化对象是最小化假反例率（优化指标），同时受到每24小时不超过一次误报的约束（满意度指标）。

一旦你的团队在优化评估指标上保持一致，他们将能够取得更快的进展。
